{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsunami Prediction Model\n",
    "\n",
    "This notebook builds a machine learning model to predict tsunamis based on historical data. We'll analyze various features like earthquake magnitude, location, and other geological factors to predict tsunami occurrence and characteristics.\n",
    "\n",
    "## Dataset Overview\n",
    "The dataset contains historical tsunami events with various features including:\n",
    "- Earthquake magnitude and location\n",
    "- Tsunami characteristics (height, magnitude, intensity)\n",
    "- Geographic information (latitude, longitude, country)\n",
    "- Impact data (deaths, damage, houses affected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Sklearn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tsunami dataset\n",
    "df = pd.read_csv('Tsunami.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {len(df.columns)}\")\n",
    "print(\"\\nFirst few column names:\")\n",
    "print(df.columns[:15].tolist())\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"- Total records: {len(df)}\")\n",
    "print(f\"- Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "key_columns = ['Year', 'Earthquake Magnitude', 'Latitude', 'Longitude', \n",
    "               'Maximum Water Height (m)', 'Tsunami Magnitude (Abe)', 'Deaths']\n",
    "print(f\"\\nMissing values in key columns:\")\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        print(f\"- {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data:\")\n",
    "sample_cols = ['Year', 'Earthquake Magnitude', 'Latitude', 'Longitude', \n",
    "               'Maximum Water Height (m)', 'Deaths', 'Country']\n",
    "available_cols = [col for col in sample_cols if col in df.columns]\n",
    "print(df[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "print(\"=== Data Preprocessing ===\")\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Remove the first column if it's unnamed/search parameters\n",
    "if ' Search Parameters' in df_processed.columns:\n",
    "    df_processed = df_processed.drop(columns=[' Search Parameters'])\n",
    "\n",
    "# Convert year to numeric and filter for modern data (after 1900 for better quality)\n",
    "df_processed['Year'] = pd.to_numeric(df_processed['Year'], errors='coerce')\n",
    "df_processed = df_processed[df_processed['Year'] >= 1900].copy()\n",
    "\n",
    "print(f\"After filtering modern data (>= 1900): {len(df_processed)} records\")\n",
    "\n",
    "# Key features for prediction\n",
    "feature_columns = [\n",
    "    'Year', 'Earthquake Magnitude', 'Latitude', 'Longitude',\n",
    "    'Maximum Water Height (m)', 'Tsunami Magnitude (Abe)', 'Tsunami Magnitude (Iida)',\n",
    "    'Tsunami Intensity', 'Deaths', 'Damage ($Mil)', 'Houses Destroyed'\n",
    "]\n",
    "\n",
    "# Check which columns exist\n",
    "existing_features = [col for col in feature_columns if col in df_processed.columns]\n",
    "print(f\"\\nAvailable feature columns: {existing_features}\")\n",
    "\n",
    "# Create target variables\n",
    "# 1. Tsunami Severity Classification (based on water height and casualties)\n",
    "def classify_tsunami_severity(row):\n",
    "    \"\"\"Classify tsunami severity based on multiple factors\"\"\"\n",
    "    water_height = row.get('Maximum Water Height (m)', 0)\n",
    "    deaths = row.get('Deaths', 0)\n",
    "    intensity = row.get('Tsunami Intensity', 0)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    water_height = 0 if pd.isna(water_height) else water_height\n",
    "    deaths = 0 if pd.isna(deaths) else deaths\n",
    "    intensity = 0 if pd.isna(intensity) else intensity\n",
    "    \n",
    "    # Classification logic\n",
    "    if water_height >= 10 or deaths >= 1000 or intensity >= 5:\n",
    "        return 3  # Extreme\n",
    "    elif water_height >= 5 or deaths >= 100 or intensity >= 4:\n",
    "        return 2  # Major\n",
    "    elif water_height >= 2 or deaths >= 10 or intensity >= 3:\n",
    "        return 1  # Moderate\n",
    "    else:\n",
    "        return 0  # Minor\n",
    "\n",
    "df_processed['Tsunami_Severity'] = df_processed.apply(classify_tsunami_severity, axis=1)\n",
    "\n",
    "# 2. Binary classification: Destructive vs Non-destructive\n",
    "df_processed['Is_Destructive'] = ((df_processed['Deaths'].fillna(0) > 0) | \n",
    "                                  (df_processed['Damage ($Mil)'].fillna(0) > 0) |\n",
    "                                  (df_processed['Houses Destroyed'].fillna(0) > 0)).astype(int)\n",
    "\n",
    "print(f\"\\nTarget variable distributions:\")\n",
    "print(f\"Tsunami Severity (0=Minor, 1=Moderate, 2=Major, 3=Extreme):\")\n",
    "print(df_processed['Tsunami_Severity'].value_counts().sort_index())\n",
    "print(f\"\\nDestructive Tsunamis (0=No, 1=Yes):\")\n",
    "print(df_processed['Is_Destructive'].value_counts().sort_index())\n",
    "\n",
    "# Display sample of processed data\n",
    "print(f\"\\nProcessed data sample:\")\n",
    "display_cols = ['Year', 'Earthquake Magnitude', 'Latitude', 'Longitude', \n",
    "                'Maximum Water Height (m)', 'Deaths', 'Tsunami_Severity', 'Is_Destructive']\n",
    "available_display_cols = [col for col in display_cols if col in df_processed.columns]\n",
    "print(df_processed[available_display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization and Analysis\n",
    "print(\"=== Data Visualization ===\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Tsunami severity distribution\n",
    "axes[0, 0].pie(df_processed['Tsunami_Severity'].value_counts().sort_index(), \n",
    "               labels=['Minor', 'Moderate', 'Major', 'Extreme'], \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Tsunami Severity Distribution')\n",
    "\n",
    "# 2. Destructive vs Non-destructive\n",
    "axes[0, 1].pie(df_processed['Is_Destructive'].value_counts().sort_index(), \n",
    "               labels=['Non-destructive', 'Destructive'], \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Destructive vs Non-destructive Tsunamis')\n",
    "\n",
    "# 3. Tsunami events over time\n",
    "years_counts = df_processed.groupby('Year').size()\n",
    "axes[0, 2].plot(years_counts.index, years_counts.values)\n",
    "axes[0, 2].set_title('Tsunami Events Over Time')\n",
    "axes[0, 2].set_xlabel('Year')\n",
    "axes[0, 2].set_ylabel('Number of Events')\n",
    "\n",
    "# 4. Earthquake magnitude vs Water height\n",
    "valid_data = df_processed.dropna(subset=['Earthquake Magnitude', 'Maximum Water Height (m)'])\n",
    "if len(valid_data) > 0:\n",
    "    axes[1, 0].scatter(valid_data['Earthquake Magnitude'], \n",
    "                       valid_data['Maximum Water Height (m)'], \n",
    "                       alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('Earthquake Magnitude')\n",
    "    axes[1, 0].set_ylabel('Maximum Water Height (m)')\n",
    "    axes[1, 0].set_title('Earthquake Magnitude vs Water Height')\n",
    "\n",
    "# 5. Geographic distribution\n",
    "valid_geo = df_processed.dropna(subset=['Latitude', 'Longitude'])\n",
    "if len(valid_geo) > 0:\n",
    "    scatter = axes[1, 1].scatter(valid_geo['Longitude'], valid_geo['Latitude'], \n",
    "                                c=valid_geo['Tsunami_Severity'], \n",
    "                                cmap='Reds', alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Longitude')\n",
    "    axes[1, 1].set_ylabel('Latitude')\n",
    "    axes[1, 1].set_title('Geographic Distribution by Severity')\n",
    "    plt.colorbar(scatter, ax=axes[1, 1])\n",
    "\n",
    "# 6. Deaths vs Severity\n",
    "severity_deaths = df_processed.groupby('Tsunami_Severity')['Deaths'].mean().fillna(0)\n",
    "axes[1, 2].bar(range(len(severity_deaths)), severity_deaths.values)\n",
    "axes[1, 2].set_xlabel('Tsunami Severity')\n",
    "axes[1, 2].set_ylabel('Average Deaths')\n",
    "axes[1, 2].set_title('Average Deaths by Severity')\n",
    "axes[1, 2].set_xticks(range(len(severity_deaths)))\n",
    "axes[1, 2].set_xticklabels(['Minor', 'Moderate', 'Major', 'Extreme'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n=== Correlation Analysis ===\")\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_processed[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset and Data Preparation\n",
    "print(\"=== Preparing Data for PyTorch ===\")\n",
    "\n",
    "class TsunamiDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Tsunami Prediction\"\"\"\n",
    "    def __init__(self, features, targets, transform=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "            \n",
    "        return feature, target\n",
    "\n",
    "# Prepare features for modeling\n",
    "input_features = [\n",
    "    'Year', 'Earthquake Magnitude', 'Latitude', 'Longitude',\n",
    "    'Maximum Water Height (m)', 'Tsunami Magnitude (Abe)', 'Tsunami Intensity'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_input_features = [col for col in input_features if col in df_processed.columns]\n",
    "print(f\"Using features: {available_input_features}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_processed[available_input_features].copy()\n",
    "\n",
    "# Handle missing values with mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "\n",
    "# Prepare targets for both tasks\n",
    "y_severity = df_processed['Tsunami_Severity'].values\n",
    "y_destructive = df_processed['Is_Destructive'].values\n",
    "\n",
    "print(f\"Target distributions:\")\n",
    "print(f\"Severity classes: {np.bincount(y_severity)}\")\n",
    "print(f\"Destructive classes: {np.bincount(y_destructive)}\")\n",
    "\n",
    "# Split data for both tasks\n",
    "X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(\n",
    "    X_scaled, y_severity, test_size=0.2, random_state=42, stratify=y_severity)\n",
    "\n",
    "X_train_dest, X_test_dest, y_train_dest, y_test_dest = train_test_split(\n",
    "    X_scaled, y_destructive, test_size=0.2, random_state=42, stratify=y_destructive)\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Severity task - Train: {X_train_sev.shape[0]}, Test: {X_test_sev.shape[0]}\")\n",
    "print(f\"Destructive task - Train: {X_train_dest.shape[0]}, Test: {X_test_dest.shape[0]}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset_sev = TsunamiDataset(X_train_sev, y_train_sev)\n",
    "test_dataset_sev = TsunamiDataset(X_test_sev, y_test_sev)\n",
    "\n",
    "train_dataset_dest = TsunamiDataset(X_train_dest, y_train_dest)\n",
    "test_dataset_dest = TsunamiDataset(X_test_dest, y_test_dest)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader_sev = DataLoader(train_dataset_sev, batch_size=batch_size, shuffle=True)\n",
    "test_loader_sev = DataLoader(test_dataset_sev, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_dest = DataLoader(train_dataset_dest, batch_size=batch_size, shuffle=True)\n",
    "test_loader_dest = DataLoader(test_dataset_dest, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data loaders created with batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Neural Network Models\n",
    "print(\"=== Neural Network Model Definitions ===\")\n",
    "\n",
    "class TsunamiPredictor(nn.Module):\n",
    "    \"\"\"Multi-layer Neural Network for Tsunami Prediction\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=0.3):\n",
    "        super(TsunamiPredictor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class AdvancedTsunamiPredictor(nn.Module):\n",
    "    \"\"\"Advanced Neural Network with Residual Connections\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_classes=4, dropout_rate=0.3):\n",
    "        super(AdvancedTsunamiPredictor, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.hidden1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.hidden3 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        x = F.relu(self.bn1(self.input_layer(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # First residual block\n",
    "        identity = x\n",
    "        out = F.relu(self.bn2(self.hidden1(x)))\n",
    "        out = self.dropout(out)\n",
    "        x = out + identity  # Residual connection\n",
    "        \n",
    "        # Second residual block\n",
    "        identity = x\n",
    "        out = F.relu(self.bn3(self.hidden2(x)))\n",
    "        out = self.dropout(out)\n",
    "        x = out + identity  # Residual connection\n",
    "        \n",
    "        # Final layers\n",
    "        x = F.relu(self.bn4(self.hidden3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_scaled.shape[1]\n",
    "print(f\"Input size: {input_size}\")\n",
    "\n",
    "# Create models for both tasks\n",
    "models = {\n",
    "    'severity': {\n",
    "        'simple': TsunamiPredictor(input_size, [64, 32], 4, 0.3),\n",
    "        'advanced': AdvancedTsunamiPredictor(input_size, 128, 4, 0.3)\n",
    "    },\n",
    "    'destructive': {\n",
    "        'simple': TsunamiPredictor(input_size, [64, 32], 2, 0.3),\n",
    "        'advanced': AdvancedTsunamiPredictor(input_size, 128, 2, 0.3)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Move models to device\n",
    "for task in models:\n",
    "    for model_type in models[task]:\n",
    "        models[task][model_type] = models[task][model_type].to(device)\n",
    "\n",
    "print(\"Models created and moved to device\")\n",
    "\n",
    "# Print model architectures\n",
    "print(\"\\n=== Model Architectures ===\")\n",
    "print(\"Simple Model for Severity Classification:\")\n",
    "print(models['severity']['simple'])\n",
    "print(\"\\nAdvanced Model for Destructive Classification:\")\n",
    "print(models['destructive']['advanced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions and Model Training\n",
    "print(\"=== Training Functions ===\")\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=100, learning_rate=0.001, task_name=\"\"):\n",
    "    \"\"\"Train a PyTorch model with early stopping\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    best_accuracy = 0.0\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nTraining {task_name} model...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in test_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        test_accuracy = 100 * correct / total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_{task_name}_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Test Acc: {test_accuracy:.2f}%')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'best_{task_name}_model.pth'))\n",
    "    \n",
    "    return train_losses, test_accuracies, best_accuracy\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names):\n",
    "    \"\"\"Evaluate model and return detailed metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    \n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, target_names=class_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, all_predictions, all_targets\n",
    "\n",
    "# Train models for both tasks\n",
    "print(\"=== Model Training ===\")\n",
    "\n",
    "# Task 1: Tsunami Severity Classification\n",
    "print(\"\\n1. Training Tsunami Severity Classification Models\")\n",
    "severity_class_names = ['Minor', 'Moderate', 'Major', 'Extreme']\n",
    "\n",
    "# Train simple model for severity\n",
    "sev_simple_losses, sev_simple_accs, sev_simple_best = train_model(\n",
    "    models['severity']['simple'], train_loader_sev, test_loader_sev, \n",
    "    num_epochs=100, learning_rate=0.001, task_name=\"severity_simple\"\n",
    ")\n",
    "\n",
    "# Train advanced model for severity\n",
    "sev_advanced_losses, sev_advanced_accs, sev_advanced_best = train_model(\n",
    "    models['severity']['advanced'], train_loader_sev, test_loader_sev, \n",
    "    num_epochs=100, learning_rate=0.001, task_name=\"severity_advanced\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSeverity Classification Results:\")\n",
    "print(f\"Simple Model Best Accuracy: {sev_simple_best:.2f}%\")\n",
    "print(f\"Advanced Model Best Accuracy: {sev_advanced_best:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Destructive Tsunami Classification\n",
    "print(\"\\n2. Training Destructive Tsunami Classification Models\")\n",
    "destructive_class_names = ['Non-destructive', 'Destructive']\n",
    "\n",
    "# Train simple model for destructive classification\n",
    "dest_simple_losses, dest_simple_accs, dest_simple_best = train_model(\n",
    "    models['destructive']['simple'], train_loader_dest, test_loader_dest, \n",
    "    num_epochs=100, learning_rate=0.001, task_name=\"destructive_simple\"\n",
    ")\n",
    "\n",
    "# Train advanced model for destructive classification\n",
    "dest_advanced_losses, dest_advanced_accs, dest_advanced_best = train_model(\n",
    "    models['destructive']['advanced'], train_loader_dest, test_loader_dest, \n",
    "    num_epochs=100, learning_rate=0.001, task_name=\"destructive_advanced\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDestructive Classification Results:\")\n",
    "print(f\"Simple Model Best Accuracy: {dest_simple_best:.2f}%\")\n",
    "print(f\"Advanced Model Best Accuracy: {dest_advanced_best:.2f}%\")\n",
    "\n",
    "# Compare all models\n",
    "print(f\"\\n=== Final Model Comparison ===\")\n",
    "print(f\"Tsunami Severity Classification:\")\n",
    "print(f\"  - Simple Model: {sev_simple_best:.2f}%\")\n",
    "print(f\"  - Advanced Model: {sev_advanced_best:.2f}%\")\n",
    "print(f\"\\nDestructive Tsunami Classification:\")\n",
    "print(f\"  - Simple Model: {dest_simple_best:.2f}%\")\n",
    "print(f\"  - Advanced Model: {dest_advanced_best:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Prediction\n",
    "print(\"=== Detailed Model Evaluation ===\")\n",
    "\n",
    "# Evaluate best models\n",
    "print(\"\\n1. Evaluating Tsunami Severity Classification (Advanced Model)\")\n",
    "sev_acc, sev_preds, sev_targets = evaluate_model(\n",
    "    models['severity']['advanced'], test_loader_sev, severity_class_names\n",
    ")\n",
    "\n",
    "print(\"\\n2. Evaluating Destructive Tsunami Classification (Advanced Model)\")\n",
    "dest_acc, dest_preds, dest_targets = evaluate_model(\n",
    "    models['destructive']['advanced'], test_loader_dest, destructive_class_names\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Severity training curves\n",
    "axes[0, 0].plot(sev_simple_accs, label='Simple Model', alpha=0.7)\n",
    "axes[0, 0].plot(sev_advanced_accs, label='Advanced Model', alpha=0.7)\n",
    "axes[0, 0].set_title('Severity Classification - Test Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(sev_simple_losses, label='Simple Model', alpha=0.7)\n",
    "axes[0, 1].plot(sev_advanced_losses, label='Advanced Model', alpha=0.7)\n",
    "axes[0, 1].set_title('Severity Classification - Training Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Destructive training curves\n",
    "axes[1, 0].plot(dest_simple_accs, label='Simple Model', alpha=0.7)\n",
    "axes[1, 0].plot(dest_advanced_accs, label='Advanced Model', alpha=0.7)\n",
    "axes[1, 0].set_title('Destructive Classification - Test Accuracy')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(dest_simple_losses, label='Simple Model', alpha=0.7)\n",
    "axes[1, 1].plot(dest_advanced_losses, label='Advanced Model', alpha=0.7)\n",
    "axes[1, 1].set_title('Destructive Classification - Training Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create prediction function\n",
    "def predict_tsunami(earthquake_magnitude, latitude, longitude, water_height=None, \n",
    "                   tsunami_magnitude=None, intensity=None, year=2024):\n",
    "    \"\"\"Make predictions for new tsunami data\"\"\"\n",
    "    \n",
    "    # Prepare input data\n",
    "    input_data = [year, earthquake_magnitude, latitude, longitude]\n",
    "    \n",
    "    # Add optional features with defaults if not provided\n",
    "    if water_height is not None:\n",
    "        input_data.append(water_height)\n",
    "    else:\n",
    "        input_data.append(0)  # Will be imputed\n",
    "    \n",
    "    if tsunami_magnitude is not None:\n",
    "        input_data.append(tsunami_magnitude)\n",
    "    else:\n",
    "        input_data.append(0)  # Will be imputed\n",
    "        \n",
    "    if intensity is not None:\n",
    "        input_data.append(intensity)\n",
    "    else:\n",
    "        input_data.append(0)  # Will be imputed\n",
    "    \n",
    "    # Pad or truncate to match expected input size\n",
    "    while len(input_data) < input_size:\n",
    "        input_data.append(0)\n",
    "    input_data = input_data[:input_size]\n",
    "    \n",
    "    # Apply same preprocessing\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "    input_imputed = imputer.transform(input_array)\n",
    "    input_scaled = scaler.transform(input_imputed)\n",
    "    input_tensor = torch.FloatTensor(input_scaled).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    models['severity']['advanced'].eval()\n",
    "    models['destructive']['advanced'].eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        severity_output = models['severity']['advanced'](input_tensor)\n",
    "        destructive_output = models['destructive']['advanced'](input_tensor)\n",
    "        \n",
    "        severity_probs = F.softmax(severity_output, dim=1)\n",
    "        destructive_probs = F.softmax(destructive_output, dim=1)\n",
    "        \n",
    "        severity_pred = torch.argmax(severity_probs, dim=1).cpu().numpy()[0]\n",
    "        destructive_pred = torch.argmax(destructive_probs, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        severity_confidence = severity_probs.max().cpu().numpy()\n",
    "        destructive_confidence = destructive_probs.max().cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'severity_class': severity_class_names[severity_pred],\n",
    "        'severity_confidence': f\"{severity_confidence:.2%}\",\n",
    "        'is_destructive': destructive_class_names[destructive_pred],\n",
    "        'destructive_confidence': f\"{destructive_confidence:.2%}\",\n",
    "        'severity_probabilities': {name: f\"{prob:.2%}\" for name, prob in \n",
    "                                  zip(severity_class_names, severity_probs.cpu().numpy()[0])},\n",
    "        'destructive_probabilities': {name: f\"{prob:.2%}\" for name, prob in \n",
    "                                     zip(destructive_class_names, destructive_probs.cpu().numpy()[0])}\n",
    "    }\n",
    "\n",
    "# Example predictions\n",
    "print(\"\\n=== Example Predictions ===\")\n",
    "\n",
    "# Example 1: Major earthquake in Pacific\n",
    "example1 = predict_tsunami(\n",
    "    earthquake_magnitude=8.5,\n",
    "    latitude=35.0,\n",
    "    longitude=140.0,\n",
    "    water_height=8.0\n",
    ")\n",
    "print(\"Example 1 - Major Pacific Earthquake (Mag 8.5, 8m water height):\")\n",
    "print(f\"  Severity: {example1['severity_class']} ({example1['severity_confidence']})\")\n",
    "print(f\"  Destructive: {example1['is_destructive']} ({example1['destructive_confidence']})\")\n",
    "\n",
    "# Example 2: Moderate earthquake\n",
    "example2 = predict_tsunami(\n",
    "    earthquake_magnitude=7.0,\n",
    "    latitude=30.0,\n",
    "    longitude=-120.0,\n",
    "    water_height=2.0\n",
    ")\n",
    "print(\"\\nExample 2 - Moderate Earthquake (Mag 7.0, 2m water height):\")\n",
    "print(f\"  Severity: {example2['severity_class']} ({example2['severity_confidence']})\")\n",
    "print(f\"  Destructive: {example2['is_destructive']} ({example2['destructive_confidence']})\")\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "print(\"The PyTorch-based tsunami prediction system includes:\")\n",
    "print(\"1. Two neural network architectures (Simple and Advanced with residual connections)\")\n",
    "print(\"2. Multi-class severity prediction (Minor, Moderate, Major, Extreme)\")\n",
    "print(\"3. Binary destructiveness classification (Destructive vs Non-destructive)\")\n",
    "print(\"4. Early stopping and learning rate scheduling for optimal training\")\n",
    "print(\"5. Comprehensive evaluation with confusion matrices and classification reports\")\n",
    "print(\"6. Real-time prediction capability for new tsunami events\")\n",
    "\n",
    "# Save the best models\n",
    "torch.save({\n",
    "    'severity_model': models['severity']['advanced'].state_dict(),\n",
    "    'destructive_model': models['destructive']['advanced'].state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'imputer': imputer,\n",
    "    'feature_names': available_input_features,\n",
    "    'severity_classes': severity_class_names,\n",
    "    'destructive_classes': destructive_class_names\n",
    "}, 'tsunami_prediction_models.pth')\n",
    "\n",
    "print(\"\\nModels saved to 'tsunami_prediction_models.pth'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaEHLFdhSOY6TX3vT+c0ka",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
